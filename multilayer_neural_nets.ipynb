{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multilayer_neural nets.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "8cVOZmIl-Qv1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# This code is implementation of neural network having which classifies wine bottle into 3 categories \n",
        "\n",
        "---\n",
        "### Input characteristic has 13 features, the input layer consist of 13 perceptrons and since which have to classify into three categories , we have 3 output layers and their is 1 hidden layer consist of 10 perceptrons. \n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "WsXU3nsmrQzP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from sklearn.datasets import load_wine\n",
        "import sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tr_6rNjBrW30",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return(1/(1+np.exp(-z)))\n",
        "\n",
        "def derivative(z):\n",
        "  \n",
        "  \n",
        "  return(z*(1-z))\n",
        "\n",
        "def softmax(z):\n",
        "  z_exp=np.exp(z)\n",
        "  \n",
        "  probs=z_exp/(np.sum(z_exp,axis=0,keepdims=True))\n",
        "  \n",
        "  \n",
        "  return(probs)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ep2tS7FuNVN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "###--- Loading data set of wine from sklearn ---###\n",
        "\n",
        "x, y = sklearn.datasets.load_wine(150) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2J7lXztm7rC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Input and output characteristic of neural network**"
      ]
    },
    {
      "metadata": {
        "id": "p6m24e-NuNX-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "inputlayer_neurons=13\n",
        "\n",
        "outputlayer_neurons=3\n",
        "\n",
        "hiddenlayer_neurons=10\n",
        "\n",
        "num_iterations=500\n",
        "\n",
        "num_examples=178\n",
        "\n",
        "learning_rate=0.001\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F8Z3weFR0IJ9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# (normalised) randomly intializing the thetas and the bias matrix \n",
        "\n",
        "theta1=2*np.random.random(size=(inputlayer_neurons,hiddenlayer_neurons))-1  \n",
        "\n",
        "bias1=2*np.random.random(size=(1,hiddenlayer_neurons))-1      \n",
        "\n",
        "theta2=2*np.random.random(size=(hiddenlayer_neurons,outputlayer_neurons))-1      \n",
        "\n",
        "bias2=2*np.random.random(size=(1,outputlayer_neurons))-1   \n",
        "\n",
        "activation_array=np.zeros((500,1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QqSHJQgvuNbu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "037a4039-2dfe-4db0-c3ed-63f8d1406312",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529394248670,
          "user_tz": -330,
          "elapsed": 973,
          "user": {
            "displayName": "Mrinal Anand",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "112983029241933201690"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        " \n",
        "for i in range(num_iterations):\n",
        "  \n",
        "  \n",
        "  # forward propagation #\n",
        "  \n",
        "  z1=x.dot(theta1)+bias1    \n",
        "  \n",
        "  activation1=sigmoid(z1)\n",
        "  \n",
        "  z2=activation1.dot(theta2) +bias2\n",
        " \n",
        "  \n",
        "  activation2=softmax(z2)\n",
        "\n",
        "  # back propagation #\n",
        "  \n",
        "  delta3=activation2\n",
        "  \n",
        "  delta3[range(num_examples), y] -= 1\n",
        "\n",
        "  slope_output_layer = derivative(activation2)   \n",
        "  \n",
        "  slope_hidden_layer = derivative(activation1)     \n",
        "  \n",
        "  d_output = delta3\n",
        "  \n",
        "  Error_at_hidden_layer = d_output.dot(theta2.T)   \n",
        "  \n",
        "  d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer     \n",
        "  \n",
        "  theta2 += activation1.T.dot(d_output) *learning_rate   #updating parameters                       \n",
        "  \n",
        "  bias2 += np.sum(d_output, axis=0,keepdims=True) *learning_rate \n",
        "  \n",
        "  theta1 += x.T.dot(d_hiddenlayer)*learning_rate   \n",
        "  \n",
        "  bias1 += np.sum(d_hiddenlayer, axis=0,keepdims=True) *learning_rate\n",
        "  \n",
        "  activation_array[i]=np.min(activation2)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "classic=np.argmin(activation2,axis=1)\n",
        "print(classic,\"\\n\")                        #predicting the classes (type) of wine\n",
        "  \n",
        "print(\"predicting with an accuracy of\")\n",
        "print(abs(activation_array[0]))\n",
        " \n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] \n",
            "\n",
            "predicting with an accuracy of\n",
            "[0.99438202]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}